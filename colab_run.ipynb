{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Hedging in Incomplete Markets\n",
    "\n",
    "**MSc Thesis Experiment Runner**\n",
    "\n",
    "Before running:\n",
    "1. **Runtime > Change runtime type > A100 GPU**\n",
    "2. Click **Connect**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 1: Clone repo and install dependencies\n",
    "!git clone https://github.com/thabangTheActuaryCoder/deep-hedging-thesis.git\n",
    "%cd deep-hedging-thesis\n",
    "!pip install -q torch numpy matplotlib optuna\n",
    "\n",
    "import torch\n",
    "print(f\"\\nGPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Sanity check â€” all tests should pass\n",
    "!python -m pytest tests/test_validation.py -v"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 3 (QUICK TEST): ~5 min on A100, use this first to verify everything works\n",
    "!python run_experiment.py --quick"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 4 (FULL RUN): ~2-4 hours on A100\n",
    "# Comment out Cell 3 above before running this\n",
    "!python run_experiment.py \\\n",
    "    --paths 50000 \\\n",
    "    --N 200 \\\n",
    "    --epochs 1000 \\\n",
    "    --patience 15 \\\n",
    "    --batch_size 2048 \\\n",
    "    --n_trials 60 \\\n",
    "    --seeds 0 1 2 3 4 \\\n",
    "    --substeps 0 5 10"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Preview validation plots\n",
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "for img in sorted(glob.glob(\"outputs/plots_val/*.png\")):\n",
    "    print(f\"\\n--- {img} ---\")\n",
    "    display(Image(filename=img, width=700))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 6: Preview 3D plots\n",
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "for img in sorted(glob.glob(\"outputs/plots_3d/*.png\")):\n",
    "    print(f\"\\n--- {img} ---\")\n",
    "    display(Image(filename=img, width=700))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 7: Show validation metrics\n",
    "import json\n",
    "\n",
    "with open(\"outputs/val_metrics.json\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(f\"Best model: {metrics['best_model']}\\n\")\n",
    "for model, agg in metrics[\"aggregated_val_metrics\"].items():\n",
    "    cvar = agg[\"CVaR95_shortfall\"]\n",
    "    mse = agg[\"MSE\"]\n",
    "    print(f\"{model:6s}  CVaR95 = {cvar['mean']:.6f} +/- {cvar['std']:.6f}  \"\n",
    "          f\"MSE = {mse['mean']:.6f} +/- {mse['std']:.6f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 8: Download all outputs as zip\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "shutil.make_archive(\"outputs\", \"zip\", \".\", \"outputs\")\n",
    "files.download(\"outputs.zip\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}