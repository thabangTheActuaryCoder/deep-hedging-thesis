{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Deep Hedging in Incomplete Markets — GBM & Heston\n\n**MSc Thesis Experiment Runner**\n\nRuns the full deep hedging pipeline under two market dynamics:\n- **GBM** (constant volatility, calibrated to S&P 500)\n- **Heston** (stochastic volatility, calibrated to S&P 500)\n\n## Setup\n1. **Runtime → Change runtime type → A100 GPU** (Pro+ recommended)\n2. Click **Connect**\n3. Run **Cell 1** (clone + install)\n\n## Two ways to run\n- **Option A (Browser):** Run cells directly in this notebook\n- **Option B (VS Code):** Run Cell 2 to get an SSH tunnel, then connect VS Code and use the terminal\n\n## Checkpoint / Resume\nAll progress is automatically checkpointed (Optuna trials in SQLite, per-seed metrics, cached features). If the runtime disconnects mid-run:\n1. Reconnect and re-run **Cell 1** (re-clone + install)\n2. Re-run the **same experiment cell** — it skips completed work and picks up where it left off\n\n## Safe practice for long runs\nFor multi-hour or overnight training:\n- **Print/log progress regularly.** The pipeline prints each Optuna trial, seed, and stage as it completes — watch the output to monitor progress.\n- **Save checkpoints to Google Drive.** Copy the `outputs/` directory to Drive periodically so results survive runtime recycling (see Cell 2b below).\n- **Assume sessions can still end and plan to resume.** Even with Pro+, Colab may reclaim GPUs after ~12 hours. The checkpoint system ensures no work is lost — just reconnect and re-run."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 1: Clone repo and install dependencies\n!git clone https://github.com/thabangTheActuaryCoder/deep-hedging-thesis.git\n%cd deep-hedging-thesis\n!pip install -q torch numpy matplotlib optuna sqlalchemy\n\nimport torch\nprint(f'\\nPython: {__import__(\"sys\").version}')\nprint(f'PyTorch: {torch.__version__}')\nprint(f'GPU available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'Device: {torch.cuda.get_device_name(0)}')\n    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f'Memory: {mem:.1f} GB')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 2b: Backup & Restore — GitHub (primary) + local + Google Drive (Colab)\n# Works on both laptop and Colab. GitHub is the single source of truth.\n# Local backup: ~/deep_hedging_backup (survives repo re-clone)\n\nimport shutil, os, subprocess, datetime, pathlib\n\n# ── Auto-detect environment ─────────────────────────────────────────\nON_COLAB = os.path.exists('/content')\nREPO_DIR = '/content/deep-hedging-thesis' if ON_COLAB else os.getcwd()\nLOCAL_OUTPUTS = os.path.join(REPO_DIR, 'outputs')\nLOCAL_BACKUP = os.path.join(str(pathlib.Path.home()), 'deep_hedging_backup')\nGH_BRANCH = 'experiment-outputs'\n\n# ── Google Drive (Colab only) ──────────────────────────────────────\nDRIVE_BACKUP = None\nif ON_COLAB:\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive')\n        DRIVE_BACKUP = '/content/drive/MyDrive/deep_hedging_outputs'\n        print('Google Drive mounted.')\n    except Exception as e:\n        print(f'Google Drive unavailable: {e}')\n\n# ── GitHub token ───────────────────────────────────────────────────\n# Colab: add GITHUB_TOKEN in Colab Secrets (key icon)\n# Laptop: export GITHUB_TOKEN=ghp_... in your shell\n_gh_token = os.environ.get('GITHUB_TOKEN', '')\nif not _gh_token:\n    try:\n        from google.colab import userdata\n        _gh_token = userdata.get('GITHUB_TOKEN')\n    except Exception:\n        pass\n\nif _gh_token:\n    _repo_url = f'https://{_gh_token}@github.com/thabangTheActuaryCoder/deep-hedging-thesis.git'\n    subprocess.run(['git', 'remote', 'set-url', 'origin', _repo_url],\n                   cwd=REPO_DIR, capture_output=True)\n    subprocess.run(['git', 'config', 'user.email', 'experiment@deep-hedging.run'],\n                   cwd=REPO_DIR, capture_output=True)\n    subprocess.run(['git', 'config', 'user.name', 'Experiment Runner'],\n                   cwd=REPO_DIR, capture_output=True)\n    print(f'GitHub configured (backup branch: {GH_BRANCH})')\nelse:\n    print('No GITHUB_TOKEN found — GitHub backup disabled.')\n    print('  Set it: export GITHUB_TOKEN=ghp_...')\n\n\n# ── Backup functions ────────────────────────────────────────────────\n\ndef backup_to_local():\n    \"\"\"Copy outputs/ to ~/deep_hedging_backup (survives repo re-clone).\"\"\"\n    if not os.path.exists(LOCAL_OUTPUTS):\n        print('No outputs to back up.'); return\n    if os.path.exists(LOCAL_BACKUP):\n        shutil.rmtree(LOCAL_BACKUP)\n    shutil.copytree(LOCAL_OUTPUTS, LOCAL_BACKUP)\n    n_files = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n    print(f'Backed up {n_files} files to {LOCAL_BACKUP}')\n\n\ndef backup_to_github(message=None):\n    \"\"\"Commit outputs/ to the experiment-outputs orphan branch and push.\"\"\"\n    if not _gh_token:\n        print('GitHub backup skipped — no GITHUB_TOKEN set.'); return\n    if not os.path.exists(LOCAL_OUTPUTS):\n        print('No outputs to back up.'); return\n\n    ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n    msg = message or f'Backup outputs {ts}'\n\n    cur_branch = subprocess.run(\n        ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],\n        cwd=REPO_DIR, capture_output=True, text=True\n    ).stdout.strip()\n\n    remote_check = subprocess.run(\n        ['git', 'ls-remote', '--heads', 'origin', GH_BRANCH],\n        cwd=REPO_DIR, capture_output=True, text=True\n    )\n    branch_exists = GH_BRANCH in remote_check.stdout\n\n    if branch_exists:\n        subprocess.run(['git', 'fetch', 'origin', GH_BRANCH], cwd=REPO_DIR, capture_output=True)\n        subprocess.run(['git', 'checkout', GH_BRANCH], cwd=REPO_DIR, capture_output=True)\n        subprocess.run(['git', 'reset', '--hard', f'origin/{GH_BRANCH}'], cwd=REPO_DIR, capture_output=True)\n    else:\n        subprocess.run(['git', 'checkout', '--orphan', GH_BRANCH], cwd=REPO_DIR, capture_output=True)\n        subprocess.run(['git', 'rm', '-rf', '.'], cwd=REPO_DIR, capture_output=True)\n\n    out_dest = os.path.join(REPO_DIR, 'outputs')\n    if os.path.exists(out_dest) and os.path.realpath(out_dest) != os.path.realpath(LOCAL_OUTPUTS):\n        shutil.rmtree(out_dest)\n        shutil.copytree(LOCAL_OUTPUTS, out_dest)\n\n    subprocess.run(['git', 'add', 'outputs/'], cwd=REPO_DIR, capture_output=True)\n    result = subprocess.run(\n        ['git', 'commit', '-m', msg],\n        cwd=REPO_DIR, capture_output=True, text=True\n    )\n    if result.returncode != 0 and 'nothing to commit' in (result.stdout + result.stderr):\n        print('GitHub: no new changes to push.')\n    else:\n        push = subprocess.run(\n            ['git', 'push', '-u', 'origin', GH_BRANCH, '--force'],\n            cwd=REPO_DIR, capture_output=True, text=True\n        )\n        if push.returncode == 0:\n            n_files = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n            print(f'Backed up {n_files} files to GitHub (branch: {GH_BRANCH})')\n        else:\n            print(f'GitHub push failed: {push.stderr.strip()}')\n\n    subprocess.run(['git', 'checkout', cur_branch], cwd=REPO_DIR, capture_output=True)\n    subprocess.run(['git', 'checkout', '.'], cwd=REPO_DIR, capture_output=True)\n\n\ndef backup_to_drive():\n    \"\"\"Copy outputs/ to Google Drive (Colab only).\"\"\"\n    if not DRIVE_BACKUP:\n        print('Google Drive not available (not on Colab).'); return\n    if not os.path.exists(LOCAL_OUTPUTS):\n        print('No outputs to back up.'); return\n    if os.path.exists(DRIVE_BACKUP):\n        shutil.rmtree(DRIVE_BACKUP)\n    shutil.copytree(LOCAL_OUTPUTS, DRIVE_BACKUP)\n    n_files = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n    print(f'Backed up {n_files} files to Google Drive')\n\n\ndef backup():\n    \"\"\"Back up to all available destinations.\"\"\"\n    backup_to_local()\n    backup_to_github()\n    if DRIVE_BACKUP:\n        backup_to_drive()\n\n\ndef restore_from_backup():\n    \"\"\"Restore outputs/ — tries GitHub, then local backup, then Drive.\"\"\"\n    if os.path.exists(LOCAL_OUTPUTS):\n        n = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n        print(f'Outputs already exist locally ({n} files), skipping restore.')\n        return True\n\n    # 1. Try GitHub\n    if _gh_token:\n        print('Checking GitHub for backup...')\n        remote_check = subprocess.run(\n            ['git', 'ls-remote', '--heads', 'origin', GH_BRANCH],\n            cwd=REPO_DIR, capture_output=True, text=True\n        )\n        if GH_BRANCH in remote_check.stdout:\n            subprocess.run(['git', 'fetch', 'origin', GH_BRANCH],\n                         cwd=REPO_DIR, capture_output=True)\n            result = subprocess.run(\n                ['git', 'checkout', f'origin/{GH_BRANCH}', '--', 'outputs/'],\n                cwd=REPO_DIR, capture_output=True, text=True\n            )\n            if result.returncode == 0 and os.path.exists(LOCAL_OUTPUTS):\n                n = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n                subprocess.run(['git', 'reset', 'HEAD', 'outputs/'],\n                             cwd=REPO_DIR, capture_output=True)\n                print(f'Restored {n} files from GitHub (branch: {GH_BRANCH})')\n                return True\n\n    # 2. Try local backup (~/deep_hedging_backup)\n    if os.path.exists(LOCAL_BACKUP):\n        print(f'Restoring from {LOCAL_BACKUP}...')\n        shutil.copytree(LOCAL_BACKUP, LOCAL_OUTPUTS)\n        n = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n        print(f'Restored {n} files from local backup')\n        return True\n\n    # 3. Try Google Drive\n    if DRIVE_BACKUP and os.path.exists(DRIVE_BACKUP):\n        print('Restoring from Google Drive...')\n        shutil.copytree(DRIVE_BACKUP, LOCAL_OUTPUTS)\n        n = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n        print(f'Restored {n} files from Google Drive')\n        return True\n\n    print('No backup found.')\n    return False\n\n\n# ── Auto-restore on cell run ───────────────────────────────────────\nrestore_from_backup()\nenv = 'Colab' if ON_COLAB else 'Local'\nprint(f'\\n[{env}] Backup functions ready:')\nprint(f'  backup()            — all destinations')\nprint(f'  backup_to_local()   — {LOCAL_BACKUP}')\nprint(f'  backup_to_github()  — GitHub (branch: {GH_BRANCH})')\nif DRIVE_BACKUP:\n    print(f'  backup_to_drive()   — Google Drive')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Sanity check — all tests should pass\n!python -m pytest tests/test_validation.py -v",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 4 (QUICK TEST): ~10 min on A100, verifies both GBM + Heston pipelines\n!python run_experiment.py --quick --market_model both",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 5 (FULL RUN): both GBM + Heston, 100k paths\n# Safe to re-run after disconnect — automatically resumes from checkpoints\n# n_trials=108 = exhaustive search over all configs (dedup skips repeats)\n!python run_experiment.py \\\n    --paths 100000 \\\n    --N 200 \\\n    --epochs 1000 \\\n    --patience 15 \\\n    --batch_size 2048 \\\n    --n_trials 108 \\\n    --seeds 0 1 2 3 4 \\\n    --substeps 0 5 10 \\\n    --market_model both\n\n# Auto-backup when experiment finishes (requires Cell 2b)\ntry:\n    backup()\nexcept NameError:\n    print('Tip: run Cell 2b first to enable automatic backups')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 6: Preview GBM validation plots\nfrom IPython.display import Image, display\nimport glob\n\nprint('=== GBM Validation Plots ===')\nfor img in sorted(glob.glob('outputs/gbm/plots_val/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 7: Preview Heston validation plots\nfrom IPython.display import Image, display\nimport glob\n\nprint('=== Heston Validation Plots ===')\nfor img in sorted(glob.glob('outputs/heston/plots_val/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 8: Heston stochastic volatility diagnostic plots\nfrom IPython.display import Image, display\nimport glob\n\nprint('=== Heston Stochastic Volatility Diagnostics ===')\nfor img in sorted(glob.glob('outputs/heston/plots_heston/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 9: GBM vs Heston comparison plots\nfrom IPython.display import Image, display\nimport glob\n\nprint('=== GBM vs Heston Comparison ===')\nfor img in sorted(glob.glob('outputs/comparison/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 10: 3D delta surface plots\nfrom IPython.display import Image, display\nimport glob\n\nfor label, pattern in [('GBM', 'outputs/gbm/plots_3d/*.png'),\n                       ('Heston', 'outputs/heston/plots_3d/*.png')]:\n    imgs = sorted(glob.glob(pattern))\n    if imgs:\n        print(f'\\n=== {label} 3D Delta Surfaces ===')\n        for img in imgs:\n            print(f'\\n--- {img} ---')\n            display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 11: Show validation metrics (both market models)\nimport json, os\n\nfor market in ['gbm', 'heston']:\n    path = f'outputs/{market}/val_metrics.json'\n    if not os.path.exists(path):\n        continue\n    with open(path) as f:\n        metrics = json.load(f)\n    print(f'\\n{\"=\"*50}')\n    print(f'  {market.upper()} — Best model: {metrics[\"best_model\"]}')\n    print(f'{\"=\"*50}')\n    for model, agg in metrics['aggregated_val_metrics'].items():\n        cvar = agg['CVaR95_shortfall']\n        mse = agg['MSE']\n        print(f'  {model:6s}  CVaR95 = {cvar[\"mean\"]:.6f} +/- {cvar[\"std\"]:.6f}  '\n              f'MSE = {mse[\"mean\"]:.6f} +/- {mse[\"std\"]:.6f}')\n\nsummary_path = 'outputs/metrics_summary.json'\nif os.path.exists(summary_path):\n    with open(summary_path) as f:\n        combined = json.load(f)\n    print(f'\\n{\"=\"*50}')\n    print('  COMBINED SUMMARY')\n    print(f'{\"=\"*50}')\n    for market, agg in combined.items():\n        print(f'\\n  [{market.upper()}]')\n        for model, m in agg.items():\n            cvar = m.get('CVaR95_shortfall', {})\n            if isinstance(cvar, dict):\n                print(f'    {model:6s}  CVaR95 = {cvar.get(\"mean\",0):.6f} +/- {cvar.get(\"std\",0):.6f}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "## Replot (optional)\n\nIf you want to adjust figure dimensions, colors, grids, or fonts without\nrerunning the experiment, edit the `STYLE` dict in `replot.py` and rerun\nCell 13 below. The data was saved during the experiment.",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cell 13: Regenerate comparison plots from saved data (edit STYLE in replot.py first)\n!python replot.py --data outputs/comparison/comparison_data.pt \\\n                  --metrics outputs/metrics_summary.json \\\n                  --out outputs/comparison\n\nfrom IPython.display import Image, display\nimport glob\nfor img in sorted(glob.glob('outputs/comparison/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 14: Download all outputs as zip\nimport shutil\nfrom google.colab import files\n\nshutil.make_archive('outputs', 'zip', '.', 'outputs')\nfiles.download('outputs.zip')",
   "execution_count": null,
   "outputs": []
  }
 ]
}