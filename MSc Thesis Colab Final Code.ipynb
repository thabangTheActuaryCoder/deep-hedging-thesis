{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Deep Hedging in Incomplete Markets — GBM & Heston\n\n**MSc Thesis Experiment Runner**\n\nRuns the full deep hedging pipeline under two market dynamics:\n- **GBM** (constant volatility, calibrated to S&P 500)\n- **Heston** (stochastic volatility, calibrated to S&P 500)\n\n## Setup\n1. **Runtime → Change runtime type → A100 GPU** (Pro+ recommended)\n2. Click **Connect**\n3. Run **Cell 1** (clone + install)\n\n## Two ways to run\n- **Option A (Browser):** Run cells directly in this notebook\n- **Option B (VS Code):** Run Cell 2 to get an SSH tunnel, then connect VS Code and use the terminal\n\n## Checkpoint / Resume\nAll progress is automatically checkpointed (Optuna trials in SQLite, per-seed metrics, cached features). If the runtime disconnects mid-run:\n1. Reconnect and re-run **Cell 1** (re-clone + install)\n2. Re-run the **same experiment cell** — it skips completed work and picks up where it left off\n\n## Safe practice for long runs\nFor multi-hour or overnight training:\n- **Print/log progress regularly.** The pipeline prints each Optuna trial, seed, and stage as it completes — watch the output to monitor progress.\n- **Save checkpoints to Google Drive.** Copy the `outputs/` directory to Drive periodically so results survive runtime recycling (see Cell 2b below).\n- **Assume sessions can still end and plan to resume.** Even with Pro+, Colab may reclaim GPUs after ~12 hours. The checkpoint system ensures no work is lost — just reconnect and re-run."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 1: Clone repo and install dependencies\n!git clone https://github.com/thabangTheActuaryCoder/deep-hedging-thesis.git\n%cd deep-hedging-thesis\n!pip install -q torch numpy matplotlib optuna sqlalchemy\n\nimport torch\nprint(f'\\nPython: {__import__(\"sys\").version}')\nprint(f'PyTorch: {torch.__version__}')\nprint(f'GPU available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'Device: {torch.cuda.get_device_name(0)}')\n    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f'Memory: {mem:.1f} GB')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 2b: Backup & Restore — Google Drive + GitHub\n# Run this BEFORE the experiment. On resume after disconnect it auto-restores.\n#\n# Two backup destinations (redundant):\n#   1. Google Drive  — fast, no auth setup needed\n#   2. GitHub        — survives Drive quota issues, shareable\n#\n# Restore priority: GitHub first (most recent commit), then Drive.\n\nimport shutil, os, subprocess, datetime\n\n# ── Paths ───────────────────────────────────────────────────────────\nLOCAL_OUTPUTS = '/content/deep-hedging-thesis/outputs'\nDRIVE_BACKUP  = '/content/drive/MyDrive/deep_hedging_outputs'\nGH_BRANCH     = 'experiment-outputs'\n\n# ── 1. Mount Google Drive ───────────────────────────────────────────\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# ── 2. Configure GitHub token for push access ──────────────────────\n# Option A: Colab Secrets (recommended) — add GITHUB_TOKEN in the key icon\n# Option B: Paste token directly (less secure)\ntry:\n    from google.colab import userdata\n    _gh_token = userdata.get('GITHUB_TOKEN')\nexcept (ImportError, userdata.SecretNotFoundError):\n    _gh_token = os.environ.get('GITHUB_TOKEN', '')\n\nif _gh_token:\n    # Set remote URL with token for push access\n    _repo_url = f'https://{_gh_token}@github.com/thabangTheActuaryCoder/deep-hedging-thesis.git'\n    subprocess.run(['git', 'remote', 'set-url', 'origin', _repo_url],\n                   cwd='/content/deep-hedging-thesis', capture_output=True)\n    subprocess.run(['git', 'config', 'user.email', 'colab@experiment.run'],\n                   cwd='/content/deep-hedging-thesis', capture_output=True)\n    subprocess.run(['git', 'config', 'user.name', 'Colab Runner'],\n                   cwd='/content/deep-hedging-thesis', capture_output=True)\n    print(f'GitHub token configured (push to branch: {GH_BRANCH})')\nelse:\n    print('No GITHUB_TOKEN found — GitHub backup disabled.')\n    print('  Add it via Colab Secrets (key icon) or: os.environ[\"GITHUB_TOKEN\"] = \"ghp_...\"')\n\n\n# ── Backup functions ────────────────────────────────────────────────\n\ndef backup_to_drive():\n    \"\"\"Copy outputs/ to Google Drive.\"\"\"\n    if not os.path.exists(LOCAL_OUTPUTS):\n        print('No outputs to back up.'); return\n    if os.path.exists(DRIVE_BACKUP):\n        shutil.rmtree(DRIVE_BACKUP)\n    shutil.copytree(LOCAL_OUTPUTS, DRIVE_BACKUP)\n    n_files = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n    print(f'Backed up {n_files} files to Google Drive')\n\n\ndef backup_to_github(message=None):\n    \"\"\"Commit outputs/ to the experiment-outputs orphan branch and push.\"\"\"\n    if not _gh_token:\n        print('GitHub backup skipped — no GITHUB_TOKEN set.'); return\n    if not os.path.exists(LOCAL_OUTPUTS):\n        print('No outputs to back up.'); return\n\n    repo = '/content/deep-hedging-thesis'\n    ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M')\n    msg = message or f'Backup outputs {ts}'\n\n    # Save current branch\n    cur_branch = subprocess.run(\n        ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],\n        cwd=repo, capture_output=True, text=True\n    ).stdout.strip()\n\n    # Check if orphan branch exists on remote\n    remote_check = subprocess.run(\n        ['git', 'ls-remote', '--heads', 'origin', GH_BRANCH],\n        cwd=repo, capture_output=True, text=True\n    )\n    branch_exists = GH_BRANCH in remote_check.stdout\n\n    if branch_exists:\n        subprocess.run(['git', 'fetch', 'origin', GH_BRANCH], cwd=repo, capture_output=True)\n        subprocess.run(['git', 'checkout', GH_BRANCH], cwd=repo, capture_output=True)\n        subprocess.run(['git', 'reset', '--hard', f'origin/{GH_BRANCH}'], cwd=repo, capture_output=True)\n    else:\n        subprocess.run(['git', 'checkout', '--orphan', GH_BRANCH], cwd=repo, capture_output=True)\n        subprocess.run(['git', 'rm', '-rf', '.'], cwd=repo, capture_output=True)\n\n    # Copy outputs into repo root for this branch\n    out_dest = os.path.join(repo, 'outputs')\n    if os.path.exists(out_dest):\n        shutil.rmtree(out_dest)\n    shutil.copytree(LOCAL_OUTPUTS, out_dest)\n\n    # Stage, commit, push\n    subprocess.run(['git', 'add', 'outputs/'], cwd=repo, capture_output=True)\n    result = subprocess.run(\n        ['git', 'commit', '-m', msg],\n        cwd=repo, capture_output=True, text=True\n    )\n    if result.returncode != 0 and 'nothing to commit' in result.stdout:\n        print('GitHub: no changes to push.')\n    else:\n        push = subprocess.run(\n            ['git', 'push', '-u', 'origin', GH_BRANCH, '--force'],\n            cwd=repo, capture_output=True, text=True\n        )\n        if push.returncode == 0:\n            n_files = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n            print(f'Backed up {n_files} files to GitHub (branch: {GH_BRANCH})')\n        else:\n            print(f'GitHub push failed: {push.stderr.strip()}')\n\n    # Return to working branch\n    subprocess.run(['git', 'checkout', cur_branch], cwd=repo, capture_output=True)\n    subprocess.run(['git', 'checkout', '.'], cwd=repo, capture_output=True)\n\n\ndef backup():\n    \"\"\"Back up to both Google Drive and GitHub.\"\"\"\n    backup_to_drive()\n    backup_to_github()\n\n\ndef restore_from_backup():\n    \"\"\"Restore outputs/ — tries GitHub first, then Google Drive.\"\"\"\n    if os.path.exists(LOCAL_OUTPUTS):\n        n = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n        print(f'Outputs already exist locally ({n} files), skipping restore.')\n        return True\n\n    repo = '/content/deep-hedging-thesis'\n\n    # Try GitHub first\n    if _gh_token:\n        print('Checking GitHub for backup...')\n        remote_check = subprocess.run(\n            ['git', 'ls-remote', '--heads', 'origin', GH_BRANCH],\n            cwd=repo, capture_output=True, text=True\n        )\n        if GH_BRANCH in remote_check.stdout:\n            subprocess.run(['git', 'fetch', 'origin', GH_BRANCH], cwd=repo, capture_output=True)\n            # Extract outputs/ from that branch without switching\n            result = subprocess.run(\n                ['git', 'checkout', f'origin/{GH_BRANCH}', '--', 'outputs/'],\n                cwd=repo, capture_output=True, text=True\n            )\n            if result.returncode == 0 and os.path.exists(LOCAL_OUTPUTS):\n                n = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n                # Unstage so it doesn't interfere with main branch\n                subprocess.run(['git', 'reset', 'HEAD', 'outputs/'], cwd=repo, capture_output=True)\n                print(f'Restored {n} files from GitHub (branch: {GH_BRANCH})')\n                return True\n            print('GitHub branch exists but outputs/ not found.')\n\n    # Fall back to Google Drive\n    if os.path.exists(DRIVE_BACKUP):\n        print('Restoring from Google Drive...')\n        shutil.copytree(DRIVE_BACKUP, LOCAL_OUTPUTS)\n        n = sum(len(f) for _, _, f in os.walk(LOCAL_OUTPUTS))\n        print(f'Restored {n} files from Google Drive')\n        return True\n\n    print('No backup found on GitHub or Google Drive.')\n    return False\n\n\n# ── Auto-restore on cell run ───────────────────────────────────────\nrestore_from_backup()\nprint('\\nBackup functions ready:')\nprint('  backup()           — save to both Drive + GitHub')\nprint('  backup_to_drive()  — save to Drive only')\nprint('  backup_to_github() — save to GitHub only')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Sanity check — all tests should pass\n!python -m pytest tests/test_validation.py -v",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 4 (QUICK TEST): ~10 min on A100, verifies both GBM + Heston pipelines\n!python run_experiment.py --quick --market_model both",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 5 (FULL RUN): both GBM + Heston, 100k paths, ~4-8 hours on A100\n# Safe to re-run after disconnect — automatically resumes from checkpoints\n!python run_experiment.py \\\n    --paths 100000 \\\n    --N 200 \\\n    --epochs 1000 \\\n    --patience 15 \\\n    --batch_size 2048 \\\n    --n_trials 60 \\\n    --seeds 0 1 2 3 4 \\\n    --substeps 0 5 10 \\\n    --market_model both\n\n# Auto-backup to both Drive + GitHub when experiment finishes (requires Cell 2b)\ntry:\n    backup()\nexcept NameError:\n    print('Tip: run Cell 2b first to enable automatic backups')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 6: Preview GBM validation plots\nfrom IPython.display import Image, display\nimport glob\n\nprint('=== GBM Validation Plots ===')\nfor img in sorted(glob.glob('outputs/gbm/plots_val/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 7: Preview Heston validation plots\nfrom IPython.display import Image, display\nimport glob\n\nprint('=== Heston Validation Plots ===')\nfor img in sorted(glob.glob('outputs/heston/plots_val/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 8: Heston stochastic volatility diagnostic plots\nfrom IPython.display import Image, display\nimport glob\n\nprint('=== Heston Stochastic Volatility Diagnostics ===')\nfor img in sorted(glob.glob('outputs/heston/plots_heston/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 9: GBM vs Heston comparison plots\nfrom IPython.display import Image, display\nimport glob\n\nprint('=== GBM vs Heston Comparison ===')\nfor img in sorted(glob.glob('outputs/comparison/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 10: 3D delta surface plots\nfrom IPython.display import Image, display\nimport glob\n\nfor label, pattern in [('GBM', 'outputs/gbm/plots_3d/*.png'),\n                       ('Heston', 'outputs/heston/plots_3d/*.png')]:\n    imgs = sorted(glob.glob(pattern))\n    if imgs:\n        print(f'\\n=== {label} 3D Delta Surfaces ===')\n        for img in imgs:\n            print(f'\\n--- {img} ---')\n            display(Image(filename=img, width=700))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 11: Show validation metrics (both market models)\nimport json, os\n\nfor market in ['gbm', 'heston']:\n    path = f'outputs/{market}/val_metrics.json'\n    if not os.path.exists(path):\n        continue\n    with open(path) as f:\n        metrics = json.load(f)\n    print(f'\\n{\"=\"*50}')\n    print(f'  {market.upper()} — Best model: {metrics[\"best_model\"]}')\n    print(f'{\"=\"*50}')\n    for model, agg in metrics['aggregated_val_metrics'].items():\n        cvar = agg['CVaR95_shortfall']\n        mse = agg['MSE']\n        print(f'  {model:6s}  CVaR95 = {cvar[\"mean\"]:.6f} +/- {cvar[\"std\"]:.6f}  '\n              f'MSE = {mse[\"mean\"]:.6f} +/- {mse[\"std\"]:.6f}')\n\nsummary_path = 'outputs/metrics_summary.json'\nif os.path.exists(summary_path):\n    with open(summary_path) as f:\n        combined = json.load(f)\n    print(f'\\n{\"=\"*50}')\n    print('  COMBINED SUMMARY')\n    print(f'{\"=\"*50}')\n    for market, agg in combined.items():\n        print(f'\\n  [{market.upper()}]')\n        for model, m in agg.items():\n            cvar = m.get('CVaR95_shortfall', {})\n            if isinstance(cvar, dict):\n                print(f'    {model:6s}  CVaR95 = {cvar.get(\"mean\",0):.6f} +/- {cvar.get(\"std\",0):.6f}')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "## Replot (optional)\n\nIf you want to adjust figure dimensions, colors, grids, or fonts without\nrerunning the experiment, edit the `STYLE` dict in `replot.py` and rerun\nCell 13 below. The data was saved during the experiment.",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cell 13: Regenerate comparison plots from saved data (edit STYLE in replot.py first)\n!python replot.py --data outputs/comparison/comparison_data.pt \\\n                  --metrics outputs/metrics_summary.json \\\n                  --out outputs/comparison\n\nfrom IPython.display import Image, display\nimport glob\nfor img in sorted(glob.glob('outputs/comparison/*.png')):\n    print(f'\\n--- {img} ---')\n    display(Image(filename=img, width=700))"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 14: Download all outputs as zip\nimport shutil\nfrom google.colab import files\n\nshutil.make_archive('outputs', 'zip', '.', 'outputs')\nfiles.download('outputs.zip')",
   "execution_count": null,
   "outputs": []
  }
 ]
}